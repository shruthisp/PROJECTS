{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c41415be",
   "metadata": {},
   "source": [
    "# Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da35a6d",
   "metadata": {},
   "source": [
    "# Unsupervised Anomaly Detection Modeling\n",
    "\n",
    "\n",
    "---\n",
    "Unsupervised Learning is a machine learning technique in which the users do not need to supervise the model. Instead, it allows the model to work on its own to discover patterns and information that was previously undetected. It mainly deals with the unlabelled data. When we don't have a Y variable to predict, we are in the realm of **unsupervised learning**. Since there is no Y variable, unsupervised learning has no measurable \"goal\". Un-supervised learning can be applied to a number of important task such as manufacturing defect detection ,labelling un-labeled samples, catching outliers in a dataset and fraud detection in a bank transaction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e398b7cb",
   "metadata": {},
   "source": [
    "## 1) K-Means Algorithm to classify Fraud/Non-Fraud Credit Card Transaction\n",
    "Clustering is one of the most popular concepts in the domain of unsupervised learning. The assumption here is that the Data points that are similar tend to belong to similar groups or clusters, as determined by their distance from local centroids.<br>\n",
    "It aims to partition the observations into k-sets so as to minimize the within-cluster sum of squares. It starts with a group of randomly initialized centroids and then performs iterative calculations to optimize the position of centroids until the centroids stabilize, or the defined number of iterations is reached. <br>\n",
    "Steps involved are:\n",
    "1. Pick a value for¬†ùëò (the number of clusters to create).\n",
    "2. Initialize¬†ùëò 'centroids' (starting points). These do not have to be actual data points.\n",
    "3. Create clusters by assigning each data point to its nearest centroid.\n",
    "4. Make your clusters better. Reassign each centroid to the center of its cluster.\n",
    "5. Repeat steps 3-4 until the centroids converge and do not change across iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f647896",
   "metadata": {},
   "source": [
    "## 2) Local Outlier Factor\n",
    "The LOF algorithm is an unsupervised outlier detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outlier samples that have a substantially lower density than their neighbors.\n",
    "\n",
    "The number of neighbors considered, (parameter n_neighbors) is typically chosen 1) greater than the minimum number of objects a cluster has to contain, so that other objects can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by objects that can potentially be local outliers. In practice, such informations are generally not available, and taking n_neighbors=20 appears to work well in general."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8192149f",
   "metadata": {},
   "source": [
    "## 3) Isolation Forest\n",
    "One of the newest techniques to detect anomalies is called Isolation Forests. The algorithm is based on the fact that anomalies are data points that are few and different. As a result of these properties, anomalies are susceptible to a mechanism called isolation.\n",
    "<br>\n",
    "This method is highly useful and is fundamentally different from all existing methods. It introduces the use of isolation as a more effective and efficient means to detect anomalies than the commonly used basic distance and density measures. Moreover, this method is an algorithm with a low linear time complexity and a small memory requirement. It builds a good performing model with a small number of trees using small sub-samples of fixed size, regardless of the size of a data set.\n",
    "<br>\n",
    "Typical machine learning methods tend to work better when the patterns they try to learn are balanced, meaning the same amount of good and bad behaviors are present in the dataset.\n",
    "<br>\n",
    "How Isolation Forests Work? The Isolation Forest algorithm isolates observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. The logic argument goes: isolating anomaly observations is easier because only a few conditions are needed to separate those cases from the normal observations. On the other hand, isolating normal observations require more conditions. Therefore, an anomaly score can be calculated as the number of conditions required to separate a given observation.\n",
    "<br>\n",
    "The way that the algorithm constructs the separation is by first creating isolation trees, or random decision trees. Then, the score is calculated as the path length to isolate the observation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07aeb52",
   "metadata": {},
   "source": [
    "## 4) One Class SVM\n",
    "One-class classification techniques can be used for binary (two-class) imbalanced classification problems where the negative case (class 0) is taken as ‚Äúnormal‚Äù and the positive case (class 1) is taken as an outlier or anomaly.\n",
    "\n",
    "- Negative Case: Normal or inlier.\n",
    "- Positive Case: Anomaly or outlier.\n",
    "\n",
    "One-class SVM is a variation of the SVM that can be used in an unsupervised setting for anomaly detection.In one class problem, all data belong to a single class. In this case, the algorithm is trained to learn what is ‚Äúnormal‚Äù, so that when a new data is shown the algorithm can identify whether it should belong to the group or not. \n",
    "a regular SVM for classification finds a max-margin hyperplane that seperates the positive points from the negative ones. \n",
    "The one-class SVM finds a hyper-plane that separates the given dataset from the origin such that the hyperplane is as close to the datapoints as possible.\n",
    "one-class SVM uses a hyperparameter which is used to define what portion of data should be classified as outliers.\n",
    "The predicted dataset will have either 1 or -1 values, where -1 values are outliers detected by the algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbe8985",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20581f6",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b300dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import SVC, OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import confusion_matrix, classification_report, plot_confusion_matrix, auc, \\\n",
    "                            accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a2edb9",
   "metadata": {},
   "source": [
    "======================================================================================================================\n",
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "800e7705",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>1.345852</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.175121</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10       V11       V12       V13       V14  \\\n",
       "0  0.098698  0.363787  0.090794 -0.551600 -0.617801 -0.991390 -0.311169   \n",
       "1  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772   \n",
       "2  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946   \n",
       "3  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924   \n",
       "4 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670   \n",
       "\n",
       "        V15       V16       V17       V18       V19       V20       V21  \\\n",
       "0  1.468177 -0.470401  0.207971  0.025791  0.403993  0.251412 -0.018307   \n",
       "1  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775   \n",
       "2  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998   \n",
       "3 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300   \n",
       "4  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431   \n",
       "\n",
       "        V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "\n",
       "   Amount  Class  \n",
       "0  149.62      0  \n",
       "1    2.69      0  \n",
       "2  378.66      0  \n",
       "3  123.50      0  \n",
       "4   69.99      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df = pd.read_csv('../data/creditcard 2.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee50ecf",
   "metadata": {},
   "source": [
    "### Define X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcb69665",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = 'Class')\n",
    "y = df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6154d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline model\n",
    "\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5325517c",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a772dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba4920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fcd695",
   "metadata": {},
   "source": [
    "### Scale the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2b9fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "\n",
    "Xs_train = ss.fit_transform(X_train)\n",
    "Xs_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e641d339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale your X data without splitting\n",
    "sc = StandardScaler()\n",
    "X_scaled = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240f8d3f",
   "metadata": {},
   "source": [
    "# Unsupervised Learning Models\n",
    "Unsupervised approach to detect frauds, the only place the labels are used is to evaluate the algorithm. One of the biggest challenge of this problem is that the target is highly imbalanced as only 0.17% cases are fraudulent transactions. But the advantage of the representation learning approach is that it is still able to handle such imbalance nature of the problems.<br>\n",
    "Un-supervised learning can be applied to a number of important task such as manufacturing defect detection ,labelling un-labeled samples, catching outliers in a dataset and fraud detection in a bank transaction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ad663a",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "81c0d738",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn : true negatives\n",
      "fp : false positives\n",
      "fn : false negatives\n",
      "tp : true positives\n",
      "=========================================================================\n",
      "Total Errros =  130255\n",
      "\n",
      "K-Means\n",
      "Confusion Matrix\n",
      "tn = 154358 fp = 129957\n",
      "fn = 298 tp = 194\n",
      "Scores\n",
      "Accuracy:  0.5426552015926575\n",
      "Precison:  0.0014905763305698766\n",
      "Recall:  0.3943089430894309\n",
      "F1:  0.002969925675313641\n",
      "=========================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      1.00      0.70    154656\n",
      "           1       0.39      0.00      0.00    130151\n",
      "\n",
      "    accuracy                           0.54    284807\n",
      "   macro avg       0.47      0.50      0.35    284807\n",
      "weighted avg       0.48      0.54      0.38    284807\n",
      "\n",
      "=========================================================================\n",
      "CPU times: user 8.65 s, sys: 607 ms, total: 9.26 s\n",
      "Wall time: 3.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Instantiate and fit Kmeans algorithm\n",
    "\n",
    "kmeans=KMeans(n_clusters=2,algorithm=\"auto\",max_iter=10000)\n",
    "kmeans.fit(X_scaled)\n",
    "kmeans_predicted_labels=kmeans.predict(X_scaled)\n",
    "\n",
    "print(\"tn : true negatives\")\n",
    "print(\"fp : false positives\")\n",
    "print(\"fn : false negatives\")\n",
    "print(\"tp : true positives\")\n",
    "tn,fp,fn,tp=confusion_matrix(y,kmeans_predicted_labels).ravel()\n",
    "reassignflag=False\n",
    "if tn+tp<fn+fp:\n",
    "    # clustering is opposite of original classification\n",
    "    reassignflag=True\n",
    "\n",
    "# Predict on Xs_test\n",
    "\n",
    "preds_kmean=kmeans.predict(X_scaled)\n",
    "\n",
    "\n",
    "if reassignflag:\n",
    "    preds_kmean=1-preds_kmean\n",
    "#calculating confusion matrix for kmeans\n",
    "tn,fp,fn,tp=confusion_matrix(y,preds_kmean).ravel()\n",
    "# plot_confusion_matrix(kmeans, Xs_test, y_test, cmap='plasma', values_format='d');\n",
    "#scoring kmeans\n",
    "kmeans_accuracy_score=accuracy_score(y,preds_kmean)\n",
    "kmeans_precison_score=precision_score(y,preds_kmean)\n",
    "kmeans_recall_score=recall_score(y,preds_kmean)\n",
    "kmeans_f1_score=f1_score(y,preds_kmean)\n",
    "\n",
    "errors_kmean = (preds_kmean != y).sum() # Total number of errors is calculated.\n",
    "print('=========================================================================')\n",
    "print('Total Errros = ' ,errors_kmean)\n",
    "#printing\n",
    "print(\"\")\n",
    "print(\"K-Means\")\n",
    "print(\"Confusion Matrix\")\n",
    "print(\"tn =\",tn,\"fp =\",fp)\n",
    "print(\"fn =\",fn,\"tp =\",tp)\n",
    "print(\"Scores\")\n",
    "print(\"Accuracy: \",kmeans_accuracy_score)\n",
    "print(\"Precison: \",kmeans_precison_score)\n",
    "print(\"Recall: \",kmeans_recall_score)\n",
    "print(\"F1: \",kmeans_f1_score)\n",
    "\n",
    "print('=========================================================================')\n",
    "print(classification_report(preds_kmean,y))\n",
    "print('=========================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0d3e02",
   "metadata": {},
   "source": [
    "---\n",
    "## Local Outlier Factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9bd734d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0 = Non_fraud: (284315, 31)\n",
      "class 1 = Fraud: (492, 31)\n",
      "Outlier_fraction:  0.0017304750013189597\n"
     ]
    }
   ],
   "source": [
    "# Outlier fraction Calculation\n",
    "\n",
    "Non_fraud = df[df['Class'] == 0]\n",
    "Fraud = df[df['Class'] == 1]  # print the shape of the class\n",
    "print('class 0 = Non_fraud:', Non_fraud.shape)\n",
    "print('class 1 = Fraud:', Fraud.shape)\n",
    "outlier_fraction = len(Fraud)/float(len(Non_fraud))\n",
    "print(\"Outlier_fraction: \", outlier_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2a096b71",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================\n",
      "985\n",
      "Accuracy =  0.9965415175891043\n",
      "Precision =  0.0\n",
      "Recall =  0.0\n",
      "F1 Score =  0.0\n",
      "=========================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    284314\n",
      "           1       0.00      0.00      0.00       493\n",
      "\n",
      "    accuracy                           1.00    284807\n",
      "   macro avg       0.50      0.50      0.50    284807\n",
      "weighted avg       1.00      1.00      1.00    284807\n",
      "\n",
      "=========================================================================\n",
      "CPU times: user 26min 59s, sys: 9min 1s, total: 36min\n",
      "Wall time: 32min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Instantiate LOF\n",
    "#  contamination is the proportion of outliers in the data set or outlier fraction\n",
    "lof = LocalOutlierFactor(n_neighbors = 20, contamination = outlier_fraction)\n",
    "\n",
    "# Fitting the model.(fit_predict when novelty is false)\n",
    "y_pred_lof = lof.fit_predict(X_scaled) \n",
    "\n",
    "# # Prediction using trained model.(When novelty is true)\n",
    "# y_pred_lof = lof.predict(X_scaled)\n",
    "\n",
    "y_pred_lof[y_pred_lof == 1] = 0 # Valid transactions are labelled as 0.\n",
    "y_pred_lof[y_pred_lof == -1] = 1 # Fraudulent transactions are labelled as 1.\n",
    "errors_lof = (y_pred_lof != y).sum() # Total number of errors is calculated.\n",
    "print('=========================================================================')\n",
    "print(errors_lof)\n",
    "print(\"Accuracy = \", accuracy_score(y_pred_lof,y))\n",
    "print(\"Precision = \", precision_score(y,y_pred_lof))\n",
    "print(\"Recall = \", recall_score(y,y_pred_lof))\n",
    "print(\"F1 Score = \", f1_score(y,y_pred_lof))\n",
    "print('=========================================================================')\n",
    "print(classification_report(y_pred_lof,y))\n",
    "print('=========================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36733cb",
   "metadata": {},
   "source": [
    "We can see the binary classification report with Local Outlier Factor model. The Class 0 (Non Fraud) has higher precision and recall than the class 1(Fraud). \n",
    "- Recall \"how many of this class you find over the whole number of element of this class\"\n",
    "- The precision will be \"how many are correctly classified among that class\"\n",
    "- The f1-score is the harmonic mean between precision & recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a18346",
   "metadata": {},
   "source": [
    "---\n",
    "## Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "546cfedc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================\n",
      "Total errors 643\n",
      "Accuracy: 0.9977423307713644\n",
      "=========================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    284314\n",
      "           1       0.35      0.35      0.35       493\n",
      "\n",
      "    accuracy                           1.00    284807\n",
      "   macro avg       0.67      0.67      0.67    284807\n",
      "weighted avg       1.00      1.00      1.00    284807\n",
      "\n",
      "CPU times: user 39.1 s, sys: 5.15 s, total: 44.2 s\n",
      "Wall time: 45.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Instantiate Isolation Forest\n",
    "iso_for = IsolationForest(max_samples = len(X),contamination = outlier_fraction)\n",
    "\n",
    "# Fitting the model.\n",
    "iso_for.fit(X_scaled)\n",
    "\n",
    "# Prediction using trained model.\n",
    "y_pred_if = iso_for.predict(X_scaled) \n",
    "y_pred_if[y_pred_if == 1] = 0 # Valid transactions are labelled as 0.\n",
    "y_pred_if[y_pred_if == -1] = 1 # Fraudulent transactions are labelled as 1.\n",
    "errors_if = (y_pred_if != y).sum() # Total number of errors is calculated.\n",
    "print('=========================================================================')\n",
    "print(\"Total errors\", errors_if)\n",
    "print('Accuracy:', accuracy_score(y_pred_if,y))\n",
    "print('=========================================================================')\n",
    "print(classification_report(y_pred_if,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5401d355",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.9977423307713644\n",
      "Precision =  0.3475609756097561\n",
      "Recall =  0.34685598377281945\n",
      "F1 Score =  0.3472081218274112\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = \", accuracy_score(y_pred_if,y))\n",
    "print(\"Precision = \", precision_score(y_pred_if,y))\n",
    "print(\"Recall = \", recall_score(y_pred_if,y))\n",
    "print(\"F1 Score = \", f1_score(y_pred_if,y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b755dd",
   "metadata": {},
   "source": [
    "---\n",
    "## One Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e7ce926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================\n",
      "Total errors 13951\n",
      "Accuracy: 0.951015951152886\n",
      "Precision =  0.8495934959349594\n",
      "Recall =  0.02924099335431969\n",
      "F1 Score =  0.05653614661527017\n",
      "=========================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97    270512\n",
      "           1       0.85      0.03      0.06     14295\n",
      "\n",
      "    accuracy                           0.95    284807\n",
      "   macro avg       0.90      0.51      0.52    284807\n",
      "weighted avg       0.95      0.95      0.93    284807\n",
      "\n",
      "CPU times: user 32min 51s, sys: 13.5 s, total: 33min 5s\n",
      "Wall time: 37min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "one_svm = OneClassSVM(kernel='rbf', degree=3, gamma=0.1,nu=0.05)\n",
    "\n",
    "# Fitting the model.\n",
    "one_svm.fit(X_scaled)\n",
    "\n",
    "# Prediction using trained model.\n",
    "y_pred_svm = one_svm.predict(X_scaled) \n",
    "y_pred_svm[y_pred_svm == 1] = 0 # Valid transactions are labelled as 0.\n",
    "y_pred_svm[y_pred_svm == -1] = 1 # Fraudulent transactions are labelled as 1.\n",
    "errors_svm = (y_pred_svm != y).sum() # Total number of errors is calculated.\n",
    "\n",
    "print('=========================================================================')\n",
    "print(\"Total errors\", errors_svm)\n",
    "print('Accuracy:', accuracy_score(y_pred_svm,y))\n",
    "print(\"Precision = \", precision_score(y_pred_svm,y))\n",
    "print(\"Recall = \", recall_score(y_pred_svm,y))\n",
    "print(\"F1 Score = \", f1_score(y_pred_svm,y))\n",
    "print('=========================================================================')\n",
    "print(classification_report(y_pred_svm,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b3e5c5",
   "metadata": {},
   "source": [
    "---\n",
    "## Results\n",
    "\n",
    "The results are presented based on three metrics: precision, recall and F-measure. Precision refers to the ability of the model to be trustworthy in regard to its classified positive points; that is, precision tells us how many of the predicted frauds are actually frauds. High precision means that when the model classifies a point as positive, it is highly likely that it is a correct classification. This metric is defined by the following equation: Precision = True Positive/(True Positive + False Positive). Recall indicates the ability of the model to detect the positive class. When a model presents a high recall, it means that the majority of positive data points are correctly identified. The equation for recall is as follows: Recall = True Positive/(True Positive + False Negative). Precision and recall indicate two opposite properties of a model, meaning that optimising one implies worsening the other. In order to gain a more comprehensive overview of the performance of the model, we can use the F-measure metric, defined as shown in the following equation: F-Measure = 2(Precision ‚àó Recall)/(Precision + Recall). These metrics are calculated for each of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5627a112",
   "metadata": {},
   "source": [
    "- Isolation Forest detected 643 errors versus Local Outlier Factor detecting 985 errors and 13951 in One Class SVM, KMeans had error 130255 (classified non fraud data as fraud transactions)\n",
    "- Isolation Forest has a 99.74% more accurate than LOF of 99.65% and SVM of 95.1%\n",
    "- When comparing error precision & recall for 3 models , the Isolation Forest performed much better than the LOF, SVM and KMeans as we can see that the detection of fraud cases is around 28 % versus LOF detection rate of just 0 for LOF and SVM of 2%.\n",
    "- Finally the model computation time was also better than the rest of the models(44secs). It too fairly less time than SVM and LOF.\n",
    "- So overall Isolation Forest Method performed much better in determining the fraud cases which is around 28%.\n",
    "- We can conclude saying that Isolation forest is a better anomaly detection algorithm than the others for the given data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c338f15d",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
